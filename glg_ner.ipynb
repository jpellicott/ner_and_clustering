{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glg_ner",
      "provenance": [],
      "mount_file_id": "1pTaR92oAlwoPUCzH-dRV6BO8vUt8tqUd",
      "authorship_tag": "ABX9TyO7v0YgP+SQFQJscz3PvDBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpellicott/ner_and_clustering/blob/main/glg_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adapted from Abhishek Thakur's tutorial https://www.youtube.com/watch?v=MqQ7rqRllIc"
      ],
      "metadata": {
        "id": "GhKiTTfJ-_2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uul0aRl-wYcF",
        "outputId": "fed7523a-3c2f-4aa0-aef0-a4e5bf5c266d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "IcI07mhau3qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/bert-base-uncased"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vntbIngdwNY7",
        "outputId": "4d3357f3-89f6-48b6-e8ed-289c1518a963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "fatal: destination path 'bert-base-uncased' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VALID_BATCH_SIZE = 8\n",
        "    EPOCHS = 10\n",
        "    BASE_MODEL_PATH = \"/content/bert-base-uncased\"\n",
        "    MODEL_PATH = \"model.bin\"\n",
        "    TRAINING_FILE = \"/content/drive/MyDrive/glg_data/ner_dataset.csv\"\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True\n",
        "    )"
      ],
      "metadata": {
        "id": "ONb7Jmpkzdl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityDataset:\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text):\n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_pos.extend([pos[i]] * input_len)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "id": "W2BbA18fyRO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "GKkYIA2IyYvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "kXYm4K1V3JMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "67WzSphA3O3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH,return_dict=False)\n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.3)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = (loss_tag + loss_pos) / 2\n",
        "\n",
        "        return tag, pos, loss"
      ],
      "metadata": {
        "id": "L-foXjv63S-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag\n",
        "\n"
      ],
      "metadata": {
        "id": "gH8haGWFymts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "meta_data = {\n",
        "    \"enc_pos\": enc_pos,\n",
        "    \"enc_tag\": enc_tag\n",
        "}\n",
        "\n",
        "joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "(\n",
        "    train_sentences,\n",
        "    test_sentences,\n",
        "    train_pos,\n",
        "    test_pos,\n",
        "    train_tag,\n",
        "    test_tag\n",
        ") = model_selection.train_test_split(sentences, pos, tag, random_state=42, test_size=0.1)\n",
        "\n",
        "train_dataset = EntityDataset(\n",
        "    texts=train_sentences, pos=train_pos, tags=train_tag\n",
        ")\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        ")\n",
        "\n",
        "valid_dataset = EntityDataset(\n",
        "    texts=test_sentences, pos=test_pos, tags=test_tag\n",
        ")\n",
        "\n",
        "valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
        ")\n",
        "\n",
        "best_loss = np.inf\n",
        "for epoch in range(config.EPOCHS):\n",
        "    train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "    test_loss = eval_fn(valid_data_loader, model, device)\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "        best_loss = test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UErAD1ZE9pzE",
        "outputId": "df77d43e-9a32-4a04-e932-fa0ed4f2e4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Some weights of the model checkpoint at /content/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 1349/1349 [14:24<00:00,  1.56it/s]\n",
            "100%|██████████| 600/600 [00:38<00:00, 15.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.2294787999830748 Valid Loss = 0.10524148179528614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1349/1349 [14:27<00:00,  1.55it/s]\n",
            "100%|██████████| 600/600 [00:38<00:00, 15.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.09346807368922623 Valid Loss = 0.09445384508309265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 545/1349 [05:51<08:39,  1.55it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data = joblib.load(\"meta.bin\")\n",
        "enc_pos = meta_data[\"enc_pos\"]\n",
        "enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "sentence = \"\"\"\n",
        "Nebraska Sandhills wagyu beef ranch and processing plant bought by private equity firm. The firm plans on adding eco tourism to the ranch's operations\n",
        "\"\"\"\n",
        "tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
        "\n",
        "sentence = sentence.split()\n",
        "print(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "test_dataset = EntityDataset(\n",
        "    texts=[sentence], \n",
        "    pos=[[0] * len(sentence)], \n",
        "    tags=[[0] * len(sentence)]\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "model.load_state_dict(torch.load(config.MODEL_PATH))\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    data = test_dataset[0]\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device).unsqueeze(0)\n",
        "    tag, pos, _ = model(**data)\n",
        "\n",
        "    print(\n",
        "        enc_tag.inverse_transform(\n",
        "            tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "    )\n",
        "    print(\n",
        "        enc_pos.inverse_transform(\n",
        "            pos.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "    )"
      ],
      "metadata": {
        "id": "JSDHKE68-KoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wHNG5Y2TyK08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kKoQ0iZsu1qW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}